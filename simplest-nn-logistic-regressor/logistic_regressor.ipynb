{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest neural network is a logistic regressor.  A logistic regression takes in values of any range by only outputs values between 0 and 1.  If X is the input and y is the desired output, then we want to find a close approximation function for the function f where\n",
    "\n",
    "y = f(X)\n",
    "\n",
    "Let's say X is the input vector with 3 components X1, X2, X3.  W is a vector of three weights.  To compute the output of the regressor, we must first do a **linear step**:\n",
    "z = X.W + b\n",
    "where b is the **bias** which shift the output by a constant value\n",
    "Next, we perform a **nonlinear step**:\n",
    "o = A(z)\n",
    "where A is a **activation function**, in this case a sigmoid function that transforms *z( to a set of values *o* that ranges from 0 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60841366]\n",
      " [0.45860596]\n",
      " [0.3262757 ]\n",
      " [0.36375058]]\n",
      "[[ 0.60841366]\n",
      " [-0.54139404]\n",
      " [-0.6737243 ]\n",
      " [ 0.36375058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "# Define input X and desired output y\n",
    "X = np.array([[0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 1, 1]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Define sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Starts with random weights and bias=0\n",
    "W = 2*np.random.random((3,1) )- 1 # Random values to have a mean of 0 and a sd of 1\n",
    "b = 0\n",
    "\n",
    "# Attempt 1:\n",
    "# Linear step\n",
    "z = X.dot(W) + b\n",
    "# Non linear step\n",
    "o = sigmoid(z)\n",
    "print (o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising Model Parameters\n",
    "We need to search for better weights and bias (collectively known as parameters) to arrive at a closer approximation function f^ of our desired function f.  \n",
    "\n",
    "We know that:\n",
    "\n",
    "y = f(X)\n",
    "\n",
    "and \n",
    "\n",
    "y^ = f^(X)\n",
    "\n",
    "We can try to find f^ by minimising:\n",
    "\n",
    "D(y, y^)\n",
    "\n",
    "where f^ belongs to H (H=**hypothesis space**)\n",
    "\n",
    "D is referred as the **loss function**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "For a binary classification problem, we can use the binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[0.23438731]\n",
      " [0.19489098]\n",
      " [0.28000314]\n",
      " [0.11304115]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "[[0.60841366]\n",
      " [0.45860596]\n",
      " [0.3262757 ]\n",
      " [0.36375058]]\n"
     ]
    }
   ],
   "source": [
    "def bce_loss(y, y_hat):\n",
    "    N = y.shape[0]\n",
    "    print(N)\n",
    "    loss = -1/N * (y*np.log(y_hat) + (1 - y)*np.log(1-y_hat))\n",
    "    return loss\n",
    "\n",
    "# Loss for our initial approximation \n",
    "print(bce_loss(y, o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Most popular optimization algorithm for neural network is the **gradient descent**.  This method requires that the loss function has a derivative with respect to the parameters that we want to optimize.  **Backpropgation** allow us to apply gradient updates to the parameters of a model.\n",
    "\n",
    "Let's say **dz** is the derivative of loss wrt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
